{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hawaiian-citation",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-parameter",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/paultimothymooney/poetry/data\n",
    "    \n",
    "    ```\n",
    "    $ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip\n",
    "$ unzip song_lyrics.zip -d ~/aiffel/lyricist/data/lyrics  #lyrics 폴더에 압축풀기\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-sunset",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-chess",
   "metadata": {},
   "source": [
    "### glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하기\n",
    "\n",
    "txt파일에서 데이터를 받아와 문장단위로 끝어서 raw_corpus에 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "meaningful-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기(문장갯수): 187088\n",
      "Examples:\n",
      " ['The first words that come out', 'And I can see this song will be about you', \"I can't believe that I can breathe without you\"]\n",
      "raw_corpus타입 <class 'list'>\n",
      "첫번재문장[0]: The first words that come out\n",
      "마지막문장[187087]: Now I can't work like this, no, with you next to me \n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "import glob\n",
    "import os\n",
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "import glob\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기(문장갯수):\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])\n",
    "\n",
    "### 문장의 갯수 추측하기\n",
    "print('raw_corpus타입',type(raw_corpus))\n",
    "print('첫번재문장[0]:',raw_corpus[0])\n",
    "print('마지막문장[187087]:',raw_corpus[187087])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-demand",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 정제\n",
    "\n",
    "앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "\n",
    "preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "\n",
    "지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거해야 된다.   \n",
    "그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외하기를 권한다.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-niagara",
   "metadata": {},
   "source": [
    "### 문장 출력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "second-guide",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이: 29\n",
      "The first words that come out\n",
      "문장길이: 41\n",
      "And I can see this song will be about you\n",
      "문장길이: 46\n",
      "I can't believe that I can breathe without you\n",
      "문장길이: 32\n",
      "But all I need to do is carry on\n",
      "문장길이: 26\n",
      "The next line I write down\n",
      "문장길이: 47\n",
      "And there's a tear that falls between the pages\n",
      "문장길이: 45\n",
      "I know that pain's supposed to heal in stages\n",
      "문장길이: 77\n",
      "But it depends which one I'm standing on I write lines down, then rip them up\n",
      "문장길이: 86\n",
      "Describing love can't be this tough I could set this song on fire, send it up in smoke\n",
      "문장길이: 57\n",
      "I could throw it in the river and watch it sink in slowly\n",
      "문장길이: 48\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "    print('문장길이:',len(sentence)) #sentence len길이\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-ethnic",
   "metadata": {},
   "source": [
    "### 한 문장을 정체화 하는 함수를 정의한다. (문자부호, 대소문자, 특수문자를 정제해준다.)\n",
    "\n",
    "preprocess_sentence()를 정의한다.\n",
    "\n",
    "\n",
    "Hi, my name is John. *(\"Hi,\" \"my\", …, \"john.\" 으로 분리됨) - 문장부호\n",
    "\n",
    "First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자\n",
    "\n",
    "He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자\n",
    "\n",
    "\"1.\" 을 막기 위해 문장 부호 양쪽에 공백을 추가 할 거고요, \"2.\" 를 막기 위해 모든 문자들을 소문자로 변환할 겁니다. \"3.\"을 막기 위해 특수문자들은 모두 제거하도록 하죠!\n",
    "    \n",
    "\n",
    "출력 형식은 <start> 한문장 <end>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "significant-attack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "import re                  # 정규표현식을 위한 Regex 지원 모듈 (문장 데이터를 정돈하기 위해) \n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "  \n",
    "    # 아래 3단계를 거쳐 sentence는 스페이스 1개를 delimeter로 하는 소문자 단어 시퀀스로 바뀝니다.\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)        # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)                  # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)  # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      # 이전 스텝에서 본 것처럼 문장 앞뒤로 <start>와 <end>를 단어처럼 붙여 줍니다\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))   # 이 문장이 어떻게 필터링되는지 확인해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-anniversary",
   "metadata": {},
   "source": [
    " 소스문장(언어 모델의 입력 문장) :  \\\\<start\\\\> 나는 밥을 먹었다   \n",
    "타켓문장(언어 모델의 출력 문장 ): 나는 밥을 먹었다 '\\\\<end\\\\>'    \n",
    "    \n",
    "    ```\n",
    "    입력이 되는 문장 : 소스 문장(Source Sentence)   X_train\n",
    "    정답 역할을 하게 될 모델의 출력 문장을 타겟 문장(Target Sentence)   y_train \n",
    "     \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-metallic",
   "metadata": {},
   "source": [
    "#### 정제된 데이터 구축하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "expanded-stretch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> and i knew the truth , when it came , would be to that effect <end>',\n",
       " '<start> at least you re attracted to me which i did not expect <end>',\n",
       " '<start> didn t think you get my number down as such <end>',\n",
       " '<start> but i never hated myself for my age so much yeah and although my pride s , yeah , not easily disturbed , yeah <end>',\n",
       " '<start> you sent me flying when you kicked me to the curb <end>',\n",
       " '<start> so with your battered jeans yeah and your beastie tee <end>',\n",
       " '<start> now i can t work like this , no , with you next to me and although my pride s , yeah , not easily disturbed , yeah <end>',\n",
       " '<start> you sent me flying when you kicked me to the curb <end>',\n",
       " '<start> so with your battered jeans yeah and your beastie tee <end>',\n",
       " '<start> now i can t work like this , no , with you next to me <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    #if len(sentence)>14 : continue\n",
    "        \n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(len(corpus))\n",
    "\n",
    "corpus[-10:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-stockholm",
   "metadata": {},
   "source": [
    "# 정제된 데이터 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-blond",
   "metadata": {},
   "source": [
    "tf.keras.preprocessing.text.Tokenizer 패키지는 정제된 데이터를 토큰화하고, 단어 사전(vocabulary 또는 dictionary라고 칭함)을 만들어주며, 데이터를 숫자로 변환까지 해준다.\n",
    "\n",
    "이 과정을 벡터화(vectorize) 라 하며, 숫자로 변환된 데이터를 텐서(tensor) 라고 부른다.      \n",
    "우리가 사용하는 텐서플로우로 만든 모델의 입출력 데이터는 실제로는 모두 이런 텐서로 변환되어 처리된다고 한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-mechanics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2    6  241 ...    0    0    0]\n",
      " [   2    8    5 ...    0    0    0]\n",
      " [   2    5   32 ...    0    0    0]\n",
      " ...\n",
      " [   2    7 1020 ...    3    0    0]\n",
      " [   2   30   31 ...    0    0    0]\n",
      " [   5   32   15 ...   10   12    3]] <keras_preprocessing.text.Tokenizer object at 0x7efc0b1a70d0>\n",
      "(175749, 15)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf    # 대망의 텐서플로우!\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 전체 단어의 개수 \n",
    "        filters=' ',    # 별도로 전처리 로직을 추가할 수 있습니다. 이번에는 사용하지 않겠습니다.\n",
    "        oov_token=\"<unk>\"  # out-of-vocabulary, 사전에 없었던 단어는 어떤 토큰으로 대체할지\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)   # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축하게 됩니다.\n",
    "\n",
    "    # 이후 tokenizer를 활용하여 모델에 입력할 데이터셋을 구축하게 됩니다.\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환합니다.\n",
    "    \n",
    "    #len15_tensor = []\n",
    "    #for txt in tensor:\n",
    "        #if len(txt) < 16:\n",
    "            #len15_tensor.append(txt)\n",
    "    \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding  메소드를 제공합니다.\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰집니다.\n",
    "    ########len15_tensor,maxlen=15, 추가########################################################\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,maxlen=15, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-sacrifice",
   "metadata": {},
   "source": [
    "# 텐서로 표현된 정제된 데이터 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-people",
   "metadata": {},
   "source": [
    "생성된 텐서 데이터를 3번째 행까지 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifteen-meditation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   6 241 426  17  66  56   3   0   0   0   0   0   0   0]\n",
      " [  2   8   5  32  64  42 334  88  27 113   7   3   0   0   0]\n",
      " [  2   5  32  15 217  17   5  32 795 258   7   3   0   0   0]\n",
      " [  2  36  24   5  90  10  47  26 836  18   3   0   0   0   0]\n",
      " [  2   6 327 431   5 732  60   3   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-southwest",
   "metadata": {},
   "source": [
    "# tokenizer에 구축된 단어 사전의 인덱스 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qualified-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "27592\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break\n",
    "        \n",
    "print(len(tokenizer.index_word))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-greece",
   "metadata": {},
   "source": [
    "# 패딩 추가하기\n",
    "\n",
    "\n",
    "\n",
    "2 -> \\<start\\>    \n",
    "3 -> \\<\\end\\>    \n",
    "0 -> \\<pad\\>     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "southern-trauma",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(175749, 15)\n",
      "[  2   6 241 426  17  66  56   3   0   0   0   0   0   0]\n",
      "[  6 241 426  17  66  56   3   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]  # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
    "tgt_input = tensor[:, 1:]    # tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "\n",
    "print(type(tensor))\n",
    "print(tensor.shape)\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-phoenix",
   "metadata": {},
   "source": [
    "### tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "wrapped-crawford",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "print(type(src_input))\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-general",
   "metadata": {},
   "source": [
    "# Step 4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "canadian-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,tgt_input, \n",
    "                                                          test_size=0.2, \n",
    "                                                          random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-arabic",
   "metadata": {},
   "source": [
    "# Step 5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "awful-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) #히든레이어1012\n",
    "\n",
    "#hidden_size = 2048\n",
    "#model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size) #히든레이어2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "different-editor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 1.34146423e-04,  3.36800586e-04, -9.82635902e-05, ...,\n",
       "          3.65857632e-05, -4.65013843e-04,  4.74011758e-04],\n",
       "        [ 1.27563675e-04,  3.24540684e-04, -3.29311297e-04, ...,\n",
       "          1.39257027e-04, -5.36090520e-04,  6.24289911e-04],\n",
       "        ...,\n",
       "        [ 2.19937880e-03,  4.33785113e-04, -1.11543224e-03, ...,\n",
       "         -4.87961166e-04, -1.81896123e-03, -4.05072089e-04],\n",
       "        [ 2.52401060e-03,  1.25462946e-04, -1.15510426e-03, ...,\n",
       "         -4.52192093e-04, -2.18216632e-03, -3.84058716e-04],\n",
       "        [ 2.75579444e-03, -2.06585522e-04, -1.18993898e-03, ...,\n",
       "         -4.00521152e-04, -2.52619525e-03, -3.37440782e-04]],\n",
       "\n",
       "       [[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 1.78171322e-04,  2.71307596e-04,  3.70785187e-04, ...,\n",
       "         -1.33288326e-04, -3.36437341e-04, -2.08348632e-04],\n",
       "        [ 3.60594393e-04,  3.31597490e-04,  5.72489575e-04, ...,\n",
       "          1.36513409e-04, -5.07371384e-04, -2.55125575e-04],\n",
       "        ...,\n",
       "        [ 2.47686473e-03,  8.24977120e-04,  1.09853085e-04, ...,\n",
       "         -2.60576286e-04, -1.05032709e-03, -1.56504302e-05],\n",
       "        [ 2.79103057e-03,  4.96607041e-04, -6.74030525e-05, ...,\n",
       "         -3.20191291e-04, -1.45477755e-03, -1.01378682e-05],\n",
       "        [ 3.01645813e-03,  1.47928164e-04, -2.22777380e-04, ...,\n",
       "         -3.51607683e-04, -1.86724705e-03,  1.55285907e-05]],\n",
       "\n",
       "       [[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 8.12938524e-05,  2.83891364e-04,  7.70773477e-05, ...,\n",
       "         -2.04505865e-04, -1.72708154e-04,  4.67899154e-05],\n",
       "        [ 4.99458110e-04,  1.31380919e-04,  1.18265933e-04, ...,\n",
       "         -1.63884819e-04, -1.29944936e-04,  8.73379831e-05],\n",
       "        ...,\n",
       "        [ 1.71623717e-03, -3.87997978e-04, -1.39609794e-03, ...,\n",
       "          1.96137553e-05, -2.03677360e-03,  5.10574573e-05],\n",
       "        [ 2.03153607e-03, -6.15033321e-04, -1.40048377e-03, ...,\n",
       "         -2.63018283e-05, -2.43140361e-03,  9.35376738e-05],\n",
       "        [ 2.27398030e-03, -8.50374519e-04, -1.40826998e-03, ...,\n",
       "         -4.17780138e-05, -2.78674136e-03,  1.47239160e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 2.37346670e-04,  1.71621592e-04,  9.27357396e-05, ...,\n",
       "         -2.36023407e-04, -5.40367328e-04,  1.20517281e-04],\n",
       "        [ 1.71938154e-04,  2.50662879e-06,  1.45324841e-04, ...,\n",
       "         -2.65175942e-04, -7.47893471e-04,  2.19918104e-04],\n",
       "        ...,\n",
       "        [ 8.82914872e-04,  1.46986888e-04, -1.19047693e-03, ...,\n",
       "          1.60524884e-04, -9.13772965e-04,  4.24430153e-04],\n",
       "        [ 1.34406262e-03,  2.16003555e-05, -1.28530897e-03, ...,\n",
       "          4.98017944e-05, -1.24362379e-03,  3.98028380e-04],\n",
       "        [ 1.75202161e-03, -1.71314445e-04, -1.34350487e-03, ...,\n",
       "         -4.60058181e-05, -1.61542161e-03,  3.79867037e-04]],\n",
       "\n",
       "       [[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 3.41225183e-04, -6.83966618e-06, -1.23806967e-04, ...,\n",
       "         -1.38497431e-04, -5.43070200e-05,  1.44245612e-04],\n",
       "        [ 5.84439724e-04, -8.12243379e-05, -1.13852315e-04, ...,\n",
       "          9.04481858e-05, -8.14219311e-05,  2.22029208e-04],\n",
       "        ...,\n",
       "        [ 7.23449630e-04, -1.23795134e-03, -3.93212686e-04, ...,\n",
       "         -2.51602731e-04, -2.27534620e-05, -1.54190187e-04],\n",
       "        [ 6.84664294e-04, -1.03167631e-03, -6.67221204e-04, ...,\n",
       "         -2.21632363e-04, -7.99070112e-05, -4.31959168e-04],\n",
       "        [ 1.02896790e-03, -8.91034084e-04, -8.58199259e-04, ...,\n",
       "         -2.33534054e-04, -3.52698989e-04, -5.66276663e-04]],\n",
       "\n",
       "       [[ 1.00790683e-04,  1.72643544e-04,  6.89882700e-06, ...,\n",
       "         -1.51398315e-04, -1.85168989e-04,  7.19983582e-05],\n",
       "        [ 2.13958047e-04,  7.11358298e-05,  2.22506569e-04, ...,\n",
       "         -4.84993914e-04, -2.93455902e-04,  2.23180847e-04],\n",
       "        [ 3.50261034e-05, -3.26661306e-04,  1.12992922e-04, ...,\n",
       "         -4.96427703e-04, -4.26202780e-04,  4.16178082e-04],\n",
       "        ...,\n",
       "        [ 2.24625436e-03, -9.36110213e-04, -8.22956674e-04, ...,\n",
       "         -7.86203716e-04, -2.12124502e-03,  1.03398525e-04],\n",
       "        [ 2.47730827e-03, -1.13059161e-03, -9.67039668e-04, ...,\n",
       "         -6.96700125e-04, -2.46325671e-03,  1.24964441e-04],\n",
       "        [ 2.63650855e-03, -1.33039395e-03, -1.08802377e-03, ...,\n",
       "         -5.93157893e-04, -2.77675083e-03,  1.69389910e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infinite-death",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accepting-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "blond-night",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 104s 151ms/step - loss: 3.6376 - val_loss: 3.2645\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 100s 146ms/step - loss: 3.1606 - val_loss: 3.0163\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 104s 152ms/step - loss: 2.9692 - val_loss: 2.8512\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 2.8275 - val_loss: 2.7163\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 105s 154ms/step - loss: 2.7050 - val_loss: 2.5947\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 105s 153ms/step - loss: 2.5922 - val_loss: 2.4820\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 105s 154ms/step - loss: 2.4892 - val_loss: 2.3828\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 107s 156ms/step - loss: 2.3929 - val_loss: 2.2879\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 107s 155ms/step - loss: 2.3030 - val_loss: 2.1999\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 106s 155ms/step - loss: 2.2174 - val_loss: 2.1149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efba47818d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "german-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 텍스트를 실제로 생성할때는 루프를 돌면서 단어 하나씩 생성해야 합니다. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장의 텐서를 입력합니다. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 우리 모델이 예측한 마지막 단어가 바로 새롭게 생성한 단어가 됩니다. \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙여 줍니다. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측해야 합니다.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환합니다. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 이것이 최종적으로 모델이 생성한 자연어 문장입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-alcohol",
   "metadata": {},
   "source": [
    "### 히든레이어 2048로 설정한 경우\n",
    "(훈련과정 실수로 지워버렸는데 다시돌리기 시간걸려서 차이점만 확인해보자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "intermediate-respondent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , <end> '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "deadly-talent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> big tigger <end> '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> big\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "collaborative-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> your hands are mine <end> '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> your hands \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "organizational-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are my love , you are my heart <end> '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "missing-policy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what the fuck is this bitch inhalin ? <end> '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "determined-shopping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> say it all <end> '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> say \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fitted-neighbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> handsome <unk> <end> '"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> handsome \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-graphics",
   "metadata": {},
   "source": [
    "### 히든레이어 1024로 설정한 겨우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "royal-month",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-desperate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> big tigger <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> big\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "olive-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> your hands are in the air <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> your hands \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "configured-brief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the one that you know <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "coordinated-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what the fuck is you ? <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what the \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "brilliant-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> say it all , <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> say \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "historic-cheese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> handsome <unk> <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> handsome \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-humidity",
   "metadata": {},
   "source": [
    "#### 나중에 다시 볼 것들\n",
    "```\n",
    "- for src_sample, tgt_sample in dataset.take(1): break\n",
    "- def __init__(self, vocab_size, embedding_size, hidden_size):'\n",
    "- tf.data.Dataset.\n",
    "- 슬라이싱\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-flexibility",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- 256배치사이즈. 히든레이어 1024일때 random_state를 2000으로 설정했을때 val_loss가 점점 증가하서 9까지 올라갔다. 20인경우에는 잘 감소하였는데 왜 올라갔는지 잘 모르겠다.    \n",
    " random_state 는 재현가능(for reproducibility)하도록 난수의 초기값을 설정해주는 것이며, 아무 숫자나 넣어주면 됩니다. shuffle=True 가 디폴트 설정이므로 생략 가능하다라고 나온다.    \n",
    " https://rfriend.tistory.com/519 [R, Python 분석과 프로그래밍의 친구 (by R Friend)]\n",
    " \n",
    " \n",
    "\n",
    "- 이번 노드에서는 그럭저럭 따라 갈수 있었는는데 소스트레인데이터와 타겟트레인 데이터를 (124960,14)의 근사값으로 변경하는데 어려움을 많이 받았다. 팀원들 덕분에 어려움을 벗어날수 있었다. \n",
    "- 날이 갈수록 노드의 텍스트가 읽혀지지가 않는다. 컨디션이 안좋아서 그런지 모르겠지만 멘탈 관리좀 해야겠다.아무튼~.                             이번 노드는 다른노드보다 수월하다고 하였다.그런데 나는 어려웠다. 무엇인지는 알것같은데 바로 생각이 나지 않거나 적용이 되지 않았다. 그래서 이전 노드 왔다 갔다하면서 프로젝트를 수행하였다.(파이썬언어도 아직 어색하다. 시간 따로 잡아서 공부좀 해라...) \n",
    "허나 이번노드를 하면서 이전에 사용했던 부분들이 많이 겹쳐서 복습하는 듯한 느낌이 들어서 나름 괜찮았다.(그래도 어려웠다. 생소한것도 많다.)\n",
    "- TextGenerator class를 지정할 때 super 개념이 있다고 한다. 같은 교육생 한분이 커뮤니티에 정보를 제공해주었다.항상 열심히 하시는 분들을 보면 감탄이 난다. \n",
    " https://dojang.io/mod/page/view.php?id=2386\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-intervention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
